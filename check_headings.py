#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ Markdown —Ñ–∞–π–ª–∞—Ö Grav CMS
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç H1-H6 –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö H1
"""

import os
import re
from pathlib import Path
from collections import defaultdict

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
PAGES_DIR = r"c:\Users\finic_rnd8rqs\OneDrive\–î–æ–∫—É–º–µ–Ω—Ç—ã\GitHub\grav\user\pages"
REPORT_FILE = "headings_report.txt"

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
issues = []
stats = {
    'files_processed': 0,
    'files_with_issues': 0,
    'multiple_h1': 0,
    'skipped_levels': 0,
    'no_h1': 0
}

def extract_frontmatter_title(content):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç title –∏–∑ YAML frontmatter (—ç—Ç–æ —Å—á–∏—Ç–∞–µ—Ç—Å—è H1)
    """
    frontmatter_match = re.match(r'^---\s*\n(.*?)\n---', content, re.DOTALL)
    if frontmatter_match:
        yaml_content = frontmatter_match.group(1)
        title_match = re.search(r'^title:\s*["\']?(.+?)["\']?\s*$', yaml_content, re.MULTILINE)
        if title_match:
            return title_match.group(1).strip()
    return None

def extract_headings(content):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (HTML –∏ Markdown)
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—É—Ä–æ–≤–µ–Ω—å, —Ç–µ–∫—Å—Ç, –ø–æ–∑–∏—Ü–∏—è)
    """
    headings = []
    
    # HTML –∑–∞–≥–æ–ª–æ–≤–∫–∏ <h1>...</h1>
    html_pattern = r'<h([1-6])[^>]*>(.*?)</h\1>'
    for match in re.finditer(html_pattern, content, re.IGNORECASE | re.DOTALL):
        level = int(match.group(1))
        text = re.sub(r'<[^>]+>', '', match.group(2))  # –£–±–∏—Ä–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Ç–µ–≥–∏
        text = text.strip()
        headings.append((level, text, match.start()))
    
    # Markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏ # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    md_pattern = r'^(#{1,6})\s+(.+)$'
    for match in re.finditer(md_pattern, content, re.MULTILINE):
        level = len(match.group(1))
        text = match.group(2).strip()
        headings.append((level, text, match.start()))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏
    headings.sort(key=lambda x: x[2])
    
    return headings

def check_heading_hierarchy(headings, file_path, has_frontmatter_title):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∏–µ—Ä–∞—Ä—Ö–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    """
    file_issues = []
    
    # –ü–æ–¥—Å—á–µ—Ç H1
    h1_count = sum(1 for level, _, _ in headings if level == 1)
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å title –≤ frontmatter, –æ–Ω —Å—á–∏—Ç–∞–µ—Ç—Å—è –∫–∞–∫ H1
    total_h1 = h1_count + (1 if has_frontmatter_title else 0)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ H1
    if total_h1 > 1:
        stats['multiple_h1'] += 1
        file_issues.append({
            'type': 'multiple_h1',
            'message': f'–ù–∞–π–¥–µ–Ω–æ {total_h1} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ H1 (–≤–∫–ª—é—á–∞—è title –≤ frontmatter)',
            'severity': 'high'
        })
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ H1
    if total_h1 == 0:
        stats['no_h1'] += 1
        file_issues.append({
            'type': 'no_h1',
            'message': '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ H1',
            'severity': 'high'
        })
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä H1 -> H3 –±–µ–∑ H2)
    prev_level = 0 if not has_frontmatter_title else 1
    
    for level, text, _ in headings:
        if level > prev_level + 1:
            stats['skipped_levels'] += 1
            file_issues.append({
                'type': 'skipped_level',
                'message': f'–ü—Ä–æ–ø—É—â–µ–Ω —É—Ä–æ–≤–µ–Ω—å: H{prev_level} -> H{level} ("{text[:50]}...")',
                'severity': 'medium'
            })
        prev_level = level
    
    return file_issues

def analyze_file(file_path):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω markdown —Ñ–∞–π–ª
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        stats['files_processed'] += 1
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º title –∏–∑ frontmatter
        frontmatter_title = extract_frontmatter_title(content)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        headings = extract_headings(content)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—é
        file_issues = check_heading_hierarchy(headings, file_path, bool(frontmatter_title))
        
        if file_issues:
            stats['files_with_issues'] += 1
            rel_path = os.path.relpath(file_path, PAGES_DIR)
            
            issues.append({
                'file': rel_path,
                'frontmatter_title': frontmatter_title,
                'headings': headings,
                'issues': file_issues
            })
        
    except Exception as e:
        print(f"[–û–®–ò–ë–ö–ê] –ê–Ω–∞–ª–∏–∑ {file_path}: {e}")

def find_markdown_files(directory):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ .md —Ñ–∞–π–ª—ã
    """
    md_files = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.md'):
                md_files.append(os.path.join(root, file))
    return md_files

def generate_report():
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö
    """
    report_path = os.path.join(os.path.dirname(PAGES_DIR), REPORT_FILE)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("–û–¢–ß–ï–¢ –û –ü–†–û–í–ï–†–ö–ï –°–¢–†–£–ö–¢–£–†–´ –ó–ê–ì–û–õ–û–í–ö–û–í\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤:           {stats['files_processed']}\n")
        f.write(f"–§–∞–π–ª–æ–≤ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏:         {stats['files_with_issues']}\n")
        f.write(f"–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ H1:            {stats['multiple_h1']}\n")
        f.write(f"–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ H1:               {stats['no_h1']}\n")
        f.write(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —É—Ä–æ–≤–Ω–∏:          {stats['skipped_levels']}\n\n")
        
        if issues:
            f.write("=" * 80 + "\n")
            f.write("–ü–†–û–ë–õ–ï–ú–ù–´–ï –§–ê–ô–õ–´\n")
            f.write("=" * 80 + "\n\n")
            
            for item in issues:
                f.write("-" * 80 + "\n")
                f.write(f"üìÑ –§–∞–π–ª: {item['file']}\n")
                f.write("-" * 80 + "\n")
                
                if item['frontmatter_title']:
                    f.write(f"Title (frontmatter): {item['frontmatter_title']}\n\n")
                
                f.write("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:\n")
                for level, text, _ in item['headings']:
                    indent = "  " * (level - 1)
                    f.write(f"{indent}H{level}: {text[:60]}\n")
                
                f.write("\n–ü—Ä–æ–±–ª–µ–º—ã:\n")
                for issue in item['issues']:
                    severity_icon = "üî¥" if issue['severity'] == 'high' else "üü°"
                    f.write(f"{severity_icon} {issue['message']}\n")
                
                f.write("\n")
        else:
            f.write("‚úÖ –ü—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ! –í—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–º–µ—é—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É.\n")
    
    return report_path

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    """
    print("=" * 80)
    print("[–ê–ù–ê–õ–ò–ó] –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ Grav CMS")
    print("=" * 80)
    print()
    
    if not os.path.exists(PAGES_DIR):
        print(f"[–û–®–ò–ë–ö–ê] –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {PAGES_DIR}")
        return
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ markdown —Ñ–∞–π–ª—ã
    print(f"[–ü–û–ò–°–ö] –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {PAGES_DIR}")
    md_files = find_markdown_files(PAGES_DIR)
    print(f"[OK] –ù–∞–π–¥–µ–Ω–æ {len(md_files)} markdown —Ñ–∞–π–ª–æ–≤")
    print()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
    print("[–ü–†–û–¶–ï–°–°] –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤...")
    for file_path in md_files:
        analyze_file(file_path)
    
    print()
    print("=" * 80)
    print("[–°–¢–ê–¢–ò–°–¢–ò–ö–ê]")
    print("=" * 80)
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤:           {stats['files_processed']}")
    print(f"–§–∞–π–ª–æ–≤ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏:         {stats['files_with_issues']}")
    print(f"–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ H1:            {stats['multiple_h1']}")
    print(f"–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ H1:               {stats['no_h1']}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —É—Ä–æ–≤–Ω–∏:          {stats['skipped_levels']}")
    print()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report_path = generate_report()
    print(f"[–û–¢–ß–ï–¢] –°–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
    print()
    
    # –í—ã–≤–æ–¥–∏–º —Ç–æ–ø –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if issues:
        print("[–ü–†–û–ë–õ–ï–ú–´] –¢–æ–ø-10 –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:")
        print("-" * 80)
        for i, item in enumerate(issues[:10], 1):
            issue_count = len(item['issues'])
            print(f"{i}. {item['file']} ({issue_count} –ø—Ä–æ–±–ª–µ–º)")
        print()
    else:
        print("[OK] –ü—Ä–æ–±–ª–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
        print()
    
    print("[–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò]")
    print("   - –£ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω H1")
    print("   - Title –≤ frontmatter —Å—á–∏—Ç–∞–µ—Ç—Å—è –∫–∞–∫ H1")
    print("   - –ù–µ –ø—Ä–æ–ø—É—Å–∫–∞–π—Ç–µ —É—Ä–æ–≤–Ω–∏ (H1 -> H2 -> H3, –∞ –Ω–µ H1 -> H3)")
    print("   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –∞ –Ω–µ —Å—Ç–∏–ª–µ–π")

if __name__ == "__main__":
    main()
